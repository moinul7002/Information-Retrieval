{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9f5d85",
   "metadata": {},
   "source": [
    "# Task C - Information Retrieval 2\n",
    "1. Consider the access to Wikipedia as data repository source. Use `beautifulsoup` program to crawl all text issued from Wikipedia pages (no need to crawl the links of these pages).\n",
    "2. Use scikit-learn library, `countVectorizer` and `tf-Idf` vectorizer to construct the tf-idf matrix representation of all documents (all identified Finish towns + Finland pages). Refine the search when different preprocessing strategies were employed such as, with and without stopwords, your own list of stopwords, lemmatization.\n",
    "3. Consider a query, `I will visit Oulu this summer and possibly Espoo`.  Write down a program that inputs the query and outputs its tf-idf representation.\n",
    "4. Write down a program that evaluates the search result of the query to each document by computing the `inner product` of the query tf-idf representation and each of the tf-idf document. `Display the documents who achieved high score`.\n",
    "5. Repeat the above reasoning when you use `countVector` instead of tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5805d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Users/moinul/anaconda3/lib/python3.11/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/moinul/anaconda3/lib/python3.11/site-packages (from wikipedia) (4.12.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/moinul/anaconda3/lib/python3.11/site-packages (from wikipedia) (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/moinul/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/moinul/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/moinul/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/moinul/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/moinul/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/moinul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/moinul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/moinul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "from urllib.error import HTTPError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb7a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a86b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction and saving completed.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Finland\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Create a list to store the extracted text\n",
    "    extracted_text = []\n",
    "\n",
    "    # Extract text from the main page content\n",
    "    main_content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    for paragraph in main_content.find_all('p'):\n",
    "        extracted_text.append(paragraph.get_text())\n",
    "\n",
    "    # Extract text from hyperlinks within the page\n",
    "    for link in main_content.find_all('a', href=True):\n",
    "        link_url = link['href']\n",
    "        if link_url.startswith(\"/wiki/\"):\n",
    "            # Construct the full URL for the linked page\n",
    "            full_link_url = f\"https://en.wikipedia.org{link_url}\"\n",
    "            # Send a request to the linked page and extract its text\n",
    "            linked_page_response = requests.get(full_link_url)\n",
    "            if linked_page_response.status_code == 200:\n",
    "                linked_soup = BeautifulSoup(linked_page_response.text, 'html.parser')\n",
    "                for paragraph in linked_soup.find_all('p'):\n",
    "                    extracted_text.append(paragraph.get_text())\n",
    "\n",
    "    # Save the extracted text to a file\n",
    "    with open('extracted_text.txt', 'w') as file:\n",
    "        file.write(\"\\n\".join(extracted_text))\n",
    "\n",
    "    print(\"Text extraction and saving completed.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838b615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file\n",
    "with open(\"extracted_text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    documents = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94e0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #removing punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    #removing stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in text.lower().split() if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    #Stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    #Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess the document\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190c89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = [i for i in preprocessed_documents if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5261da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I will visit Oulu this summer and possibly Espoo\"\n",
    "\n",
    "preprocessed_query = preprocess_text(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab132899",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41f98a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23af8c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tfidf = vectorizer.transform([preprocessed_query])\n",
    "query_tfidf = query_tfidf.toarray()\n",
    "query_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ce4f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of the highest score and its corresponding document index\n",
    "max_score_tfidf = -1\n",
    "max_score_index_tfidf = -1\n",
    "\n",
    "# Compute the inner product (dot product) between the query and each document\n",
    "for i in range(len(preprocessed_documents)):\n",
    "    document_tfidf = tfidf_matrix[i].toarray()\n",
    "    inner_product_tfidf = np.dot(query_tfidf, document_tfidf.T)\n",
    "\n",
    "    if inner_product_tfidf > max_score_tfidf:\n",
    "        max_score_tfidf = inner_product_tfidf\n",
    "        max_score_index_tfidf = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e623721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1656: univers oulu oulu univers appli scienc main campus locat oulu (Score: 0.3075)\n"
     ]
    }
   ],
   "source": [
    "# Display the document with the highest score\n",
    "if max_score_index_tfidf != -1:\n",
    "    print(f\"Document {max_score_index_tfidf + 1}: {preprocessed_documents[max_score_index_tfidf]} (Score: {max_score_tfidf[0][0]:.4f})\")\n",
    "else:\n",
    "    print(\"No matching documents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6a830",
   "metadata": {},
   "source": [
    "### CountVectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f5cc493",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1e0878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_countvec = count_vectorizer.transform([preprocessed_query])\n",
    "query_countvec = query_countvec.toarray()\n",
    "query_countvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "221c13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of the highest score and its corresponding document index\n",
    "max_score_count = -1\n",
    "max_score_index_count = -1\n",
    "\n",
    "# Compute the inner product (dot product) between the query and each document\n",
    "for i in range(len(preprocessed_documents)):\n",
    "    document_countvec = count_matrix[i].toarray()\n",
    "    inner_product_count = np.dot(query_countvec, document_countvec.T)\n",
    "\n",
    "    if inner_product_count > max_score_count:\n",
    "        max_score_count = inner_product_count\n",
    "        max_score_index_count = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4c02fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1183: second crusad finland settler sweden establish perman agricultur settlement uusimaa espoo subdivis kirkkonummi congreg oldest known document refer kirkkonummi espoo subchapt date although first document direct refer espoo late construct espoo cathedr oldest preserv build espoo mark independ espoo administr espoo part uusimaa provinc split eastern western provinc govern porvoo raseborg castl respect eastern border raseborg provinc espoo road connect import citi finland time king road pas espoo way stockholm via turku porvoo viipuri (Score: 9.0000)\n"
     ]
    }
   ],
   "source": [
    "# Display the document with the highest score\n",
    "if max_score_index_count != -1:\n",
    "    print(f\"Document {max_score_index_count + 1}: {preprocessed_documents[max_score_index_count]} (Score: {max_score_count[0][0]:.4f})\")\n",
    "else:\n",
    "    print(\"No matching documents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933f79e",
   "metadata": {},
   "source": [
    "### End of Task C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
