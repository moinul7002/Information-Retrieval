{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059b46a5",
   "metadata": {},
   "source": [
    "# Task B - Information Retrieval 1\n",
    "1. Consider an academic journal of your own choice and collect 20 abstracts using a method of your own (can be a simple manual copy-and-past operation) in a single file. Save this file in your local desk. Save also the keywords mentioned in each abstract file (if the journal allows for keywords, otherwise you may use words of title of the paper as keywords) in another separate file.\n",
    "2. Consider an information retrieval system where a keyword plays the role search query. Write a script that uses logical query-matching for five queries of your own choice (from the list of keywords) to find out whether a given query is found in the document or not, so that for each keyword input, the program outputs 1 if a logical matching is found (the given keyword is found in the abstract) and 0, otherwise.\n",
    "3. Now instead, of compiling the abstracts into a single file, we want to keep each abstract as a separate file, labeled as A1, A2, â€¦, A20. Write a script that constructs an inverted file of the abstract files. Then suggest a program that employs a simple string matching operation to output the list of files (abstract-file (A0, A1,..A20)) for each keyword.\n",
    "4. We want to relax the assumption of exact matching between keywords and words of the abstract and allow the matching to be considered correct if 90% of the characters of the keywords are found in one word of the abstract. Write a script that implements this reasoning and display the result of your search operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9236d",
   "metadata": {},
   "source": [
    "## Solution B1:\n",
    "Consider an academic journal of your own choice and collect 20 abstracts using a method of your own (can be a simple manual copy-and-past operation) in a single file. Save this file in your local desk. Save also the keywords mentioned in each abstract file (if the journal allows for keywords, otherwise you may use words of title of the paper as keywords) in another separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbda7e",
   "metadata": {},
   "source": [
    "For this project, I will use one of my favorite journals \"ArXiv\". From the page \"https://arxiv.org/list/astro-ph.GA/current\" I will recursively crawl 20 paper details including abstract and keywords in the following python program. First it will fetch the page and save the link to details of each papers in \"paper_links.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0935e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://arxiv.org/list/astro-ph.GA/current\n",
      "https://arxiv.org/abs/2309.00031\n",
      "https://arxiv.org/abs/2309.00039\n",
      "https://arxiv.org/abs/2309.00041\n",
      "https://arxiv.org/abs/2309.00045\n",
      "https://arxiv.org/abs/2309.00048\n",
      "https://arxiv.org/abs/2309.00050\n",
      "https://arxiv.org/abs/2309.00053\n",
      "https://arxiv.org/abs/2309.00102\n",
      "https://arxiv.org/abs/2309.00110\n",
      "https://arxiv.org/abs/2309.00198\n",
      "https://arxiv.org/abs/2309.00272\n",
      "https://arxiv.org/abs/2309.00291\n",
      "https://arxiv.org/abs/2309.00318\n",
      "https://arxiv.org/abs/2309.00449\n",
      "https://arxiv.org/abs/2309.00459\n",
      "https://arxiv.org/abs/2309.00501\n",
      "https://arxiv.org/abs/2309.00657\n",
      "https://arxiv.org/abs/2309.00670\n",
      "https://arxiv.org/abs/2309.00671\n",
      "https://arxiv.org/abs/2309.00719\n",
      "https://arxiv.org/abs/2309.00852\n",
      "https://arxiv.org/abs/2309.00888\n",
      "https://arxiv.org/abs/2309.00955\n",
      "https://arxiv.org/abs/2309.01024\n",
      "https://arxiv.org/abs/2309.01039\n",
      "Total links: 25\n",
      "Saving paper links...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "root_url = \"https://arxiv.org/list/astro-ph.GA/current\"\n",
    "\n",
    "response = requests.get(root_url)\n",
    "print(\"Fetching: %s\" % root_url)\n",
    "\n",
    "links = []\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    elements = soup.select(\"#dlpage span.list-identifier a\")\n",
    "    for el in elements:\n",
    "        href = el.get('href', '')\n",
    "        if href[0:4] == \"/abs\":\n",
    "            link = \"https://arxiv.org\" + href\n",
    "            print(link)\n",
    "            links.append(link)\n",
    "print(\"Total links: %s\" % len(links))\n",
    "\n",
    "with open(\"paper_links.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(links))\n",
    "    \n",
    "print(\"Saving paper links...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72080f2f",
   "metadata": {},
   "source": [
    "Now we will load the `paper_links.txt` file and fetch details of each papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9b47bf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://arxiv.org/abs/2309.00031\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00039\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00041\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00045\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00048\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00050\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00053\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00102\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00110\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00198\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00272\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00291\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00318\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00449\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00459\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00501\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00657\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00670\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00671\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00719\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00852\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00888\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.00955\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.01024\n",
      "\n",
      "Fetching: https://arxiv.org/abs/2309.01039\n",
      "Total papers fetched: 25\n",
      "Saving abstracts.txt...\n",
      "Saving titles.txt...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "with open('paper_links.txt', 'r') as file:\n",
    "    paper_links = file.readlines()\n",
    "\n",
    "journal_data = []\n",
    "abstracts = []\n",
    "titles = []\n",
    "for paper_link in paper_links:\n",
    "    response = requests.get(paper_link)\n",
    "    print(\"Fetching: %s\" % paper_link)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        title_el = soup.select(\"#abs h1.title\")\n",
    "        abstract_el = soup.select(\"blockquote.abstract\")\n",
    "        \n",
    "        title = title_el[0].text[6:]\n",
    "        abstract = abstract_el[0].text[12:].replace('\\n', ' ')\n",
    "        \n",
    "        titles.append(title)\n",
    "        abstracts.append(abstract)\n",
    "        \n",
    "        journal_data.append({\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        })\n",
    "        \n",
    "        # save this data for backup purpose as json\n",
    "        with open(\"journal_data.json\", \"w\") as file:\n",
    "            json.dump(journal_data, file, indent=4)\n",
    "            \n",
    "print(\"Total papers fetched: %s\" % len(titles))\n",
    "            \n",
    "print(\"Saving abstracts.txt...\")\n",
    "with open(\"abstracts.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(abstracts))\n",
    "\n",
    "print(\"Saving titles.txt...\")\n",
    "with open(\"titles.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(titles))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0206d3",
   "metadata": {},
   "source": [
    "Now we will load the `titles.txt` file to process it using nltk to extract keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "786d0d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEARLS: Near Infrared Photometry in the JWST North Ecliptic Pole Time Domain Field\n",
      " -> pearls,infrared,photometry,jwst,north,ecliptic,pole,time,domain,field\n",
      "Fuzzy dark matter dynamics in tidally perturbed dwarf spheroidal galaxy satellites\n",
      " -> fuzzy,dark,matter,dynamics,perturbed,dwarf,spheroidal,galaxy,satellites\n",
      "EDGE -- Dark matter or astrophysics? Clear prospects to break dark matter heating degeneracies with HI rotation in faint dwarf galaxies\n",
      " -> edge,dark,matter,astrophysics,clear,prospects,break,dark,matter,heating,degeneracies,hi,rotation,faint,dwarf,galaxies\n",
      "The energy distribution of the first supernovae\n",
      " -> energy,distribution,first,supernovae\n",
      "Detection of the Keplerian decline in the Milky Way rotation curve\n",
      " -> detection,keplerian,decline,milky,way,rotation,curve\n",
      "Illuminating the Dark Side of Cosmic Star Formation II. A second date with RS-NIRdark galaxies in COSMOS\n",
      " -> illuminating,dark,side,cosmic,star,formation,ii,second,date,rs-nirdark,galaxies,cosmos\n",
      "The first comprehensive study of a giant nebula around a radio-quiet quasar in the $z < 1$ Universe\n",
      " -> first,comprehensive,study,giant,nebula,radio-quiet,quasar,universe\n",
      "The LOFAR Two-Metre Sky Survey (LoTSS): VI. Optical identifications for the second data release\n",
      " -> lofar,two-metre,sky,survey,lotss,vi,optical,identifications,second,data,release\n",
      "Optical- & UV-Continuum Morphologies of Compact Radio Source Hosts\n",
      " -> optical-,uv-continuum,morphologies,compact,radio,source,hosts\n",
      "Flux evolution and kinematics of superluminal components in blazar 3C345\n",
      " -> flux,evolution,kinematics,superluminal,components,blazar\n",
      "Discovery of compact disc Galaxies with High Surface Brightness in the Sloan Digital Sky Survey\n",
      " -> discovery,compact,disc,galaxies,high,surface,brightness,sloan,digital,sky,survey\n",
      "Ultraluminous Quasars At High Redshift Show Evolution In Their Radio-Loudness Fraction In Both Redshift And Ultraviolet Luminosity\n",
      " -> ultraluminous,quasars,high,redshift,show,evolution,radio-loudness,fraction,redshift,ultraviolet,luminosity\n",
      "Gas phase Elemental abundances in Molecular cloudS (GEMS). IX. Deuterated compounds of H2S in starless cores\n",
      " -> gas,phase,elemental,abundances,molecular,clouds,gems,ix,deuterated,compounds,h2s,starless,cores\n",
      "The evolution of the cold gas fraction in nearby clusters ram-pressure stripped galaxies\n",
      " -> evolution,cold,gas,fraction,nearby,clusters,ram-pressure,stripped,galaxies\n",
      "Faint [CI](1-0) emission in z $\\sim$ 3.5 radio galaxies\n",
      " -> faint,ci,1-0,emission,z,\\sim,radio,galaxies\n",
      "FAUST X: Formaldehyde in the Protobinary System [BHB2007] 11: Small Scale Deuteration\n",
      " -> faust,x,formaldehyde,protobinary,system,bhb2007,],small,scale,deuteration\n",
      "FLASHING: Project Overview\n",
      " -> flashing,project,overview\n",
      "The black hole mass metallicity relation and insights into galaxy quenching\n",
      " -> black,hole,mass,metallicity,relation,insights,galaxy,quenching\n",
      "Dark Energy Survey Year 6 Results: Intra-Cluster Light from Redshift 0.2 to 0.5\n",
      " -> dark,energy,survey,year,results,intra-cluster,light,redshift\n",
      "The cluster initial mass function of the M82 disk Super Star Clusters\n",
      " -> cluster,initial,mass,function,m82,disk,super,star,clusters\n",
      "Are there higher electron densities in narrow emission line regions of Type-1 AGN than Type-2 AGN?\n",
      " -> higher,electron,densities,narrow,emission,line,regions,type-1,agn,type-2,agn\n",
      "A large population of strongly lensed faint submillimetre galaxies in future dark energy surveys inferred from JWST imaging\n",
      " -> large,population,lensed,faint,submillimetre,galaxies,future,dark,energy,surveys,inferred,jwst,imaging\n",
      "Similarity between compact extremely red objects discovered with JWST in cosmic dawn and blue-excess dust-obscured galaxies known in cosmic noon\n",
      " -> similarity,compact,red,objects,discovered,jwst,cosmic,dawn,blue-excess,dust-obscured,galaxies,known,cosmic,noon\n",
      "Spectral Libraries for Analyzing Spectra of Low-Metalicity Galaxies\n",
      " -> spectral,libraries,analyzing,spectra,low-metalicity,galaxies\n",
      "Characterize the assembly of dark matter halos with protohalo size histories: I. Redshift evolution, relation to descendant halos, and halo assembly bias -> characterize,assembly,dark,matter,halos,protohalo,size,histories,i.,redshift,evolution,relation,descendant,halos,halo,assembly,bias\n",
      "Saving keywords.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/burhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/burhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/burhan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download NLTK data if not already installed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# most pos tags except preposition and articles\n",
    "included_pos_tags = ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']\n",
    "\n",
    "keyword_lines = []\n",
    "\n",
    "# load input\n",
    "with open('titles.txt', 'r') as file:\n",
    "    titles = file.readlines()\n",
    "    \n",
    "    for title in titles:\n",
    "        # Tokenize into words\n",
    "        words = word_tokenize(title.lower())\n",
    "        # perform part-of-speech (POS) tagging\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "        keywords = []\n",
    "        \n",
    "        for word, pos in pos_tags:\n",
    "            # Check if the word is in included parts of speech tags and not a stop word\n",
    "            if pos in included_pos_tags and word.lower() not in set(stopwords.words('english')):\n",
    "                keywords.append(word)\n",
    "                \n",
    "        keyword_line = \",\".join(keywords)\n",
    "        \n",
    "        print(\"%s -> %s\" % (title, keyword_line))\n",
    "        \n",
    "        keyword_lines.append(keyword_line)\n",
    "        \n",
    "print(\"Saving keywords.txt...\")\n",
    "with open(\"keywords.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(keyword_lines))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f65bd",
   "metadata": {},
   "source": [
    "## Solution B2:\n",
    "Consider an information retrieval system where a keyword plays the role search query. Write a script that uses logical query-matching for five queries of your own choice (from the list of keywords) to find out whether a given query is found in the document or not, so that for each keyword input, the program outputs 1 if a logical matching is found (the given keyword is found in the abstract) and 0, otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cc219df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'dark matter' -> Match: 1\n",
      "Query: 'radio galaxies' -> Match: 1\n",
      "Query: 'star formation' -> Match: 1\n",
      "Query: 'distributed systems' -> Match: 0\n",
      "Query: 'spectral libraries' -> Match: 1\n"
     ]
    }
   ],
   "source": [
    "# load Keywords from the File\n",
    "with open('keywords.txt', 'r') as keywords_file:\n",
    "    keywords_list = [line.strip() for line in keywords_file]\n",
    "\n",
    "# Sample Queries\n",
    "\n",
    "queries = [\n",
    "    \"dark matter\",\n",
    "    \"radio galaxies\",\n",
    "    \"star formation\",\n",
    "    \"distributed systems\",\n",
    "    \"spectral libraries\"\n",
    "]\n",
    "\n",
    "# query matching\n",
    "def query_matching(query):\n",
    "    for keywords in keywords_list:\n",
    "        if all(keyword in keywords for keyword in query.split()):\n",
    "            return 1  # match found\n",
    "    return 0  # match not found\n",
    "\n",
    "# run sample queries\n",
    "for query in queries:\n",
    "    result = query_matching(query)\n",
    "    print(f\"Query: '{query}' -> Match: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e109e0",
   "metadata": {},
   "source": [
    "## Solution B3:\n",
    "Now instead, of compiling the abstracts into a single file, we want to keep each abstract as a separate file, labeled as A1, A2, â€¦, A20. Write a script that constructs an inverted file of the abstract files. Then suggest a program that employs a simple string matching operation to output the list of files (abstract-file (A0, A1,..A20)) for each keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10df4f",
   "metadata": {},
   "source": [
    "First we start by converting out `abstracts.txt` file to individual files such as A0, A1, A2 etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cb287394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('abstracts.txt', 'r') as abstracts_file:\n",
    "    abstracts = abstracts_file.read().splitlines()\n",
    "\n",
    "if not os.path.exists('abstracts'):\n",
    "    os.makedirs('abstracts')\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "    abstract_filename = f'abstracts/A{i}.txt'\n",
    "    with open(abstract_filename, 'w') as abstract_file:\n",
    "        abstract_file.write(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2d652",
   "metadata": {},
   "source": [
    "Now, let's construct an inverted file for keyword lookup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ff746907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/burhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/burhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/burhan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inverted_file.txt...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download NLTK data if not already installed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# most pos tags except preposition and articles\n",
    "included_pos_tags = ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']\n",
    "\n",
    "inverted_file = {}\n",
    "\n",
    "# read all files in the 'abstracts' folder\n",
    "for filename in os.listdir('abstracts'):\n",
    "    if filename.endswith('.txt'):\n",
    "        abstract_filename = os.path.join('abstracts', filename)\n",
    "        with open(abstract_filename, 'r') as abstract_file:\n",
    "            abstract_text = abstract_file.read()\n",
    "            \n",
    "            words = word_tokenize(abstract_text.lower())\n",
    "            # perform part-of-speech (POS) tagging\n",
    "            pos_tags = nltk.pos_tag(words)\n",
    "            \n",
    "            keywords = []\n",
    "            \n",
    "            for word, pos in pos_tags:\n",
    "                # Check if the word is in included parts of speech tags and not a stop word\n",
    "                if pos in included_pos_tags and word.lower() not in set(stopwords.words('english')):\n",
    "                    keywords.append(word)\n",
    "            for keyword in keywords:\n",
    "                if keyword not in inverted_file:\n",
    "                    inverted_file[keyword] = []\n",
    "                inverted_file[keyword].append(filename)\n",
    "\n",
    "# Save the inverted file\n",
    "with open('inverted_file.txt', 'w') as inverted_file_txt:\n",
    "    for keyword, abstracts_list in inverted_file.items():\n",
    "        inverted_file_txt.write(f'{keyword}: {\", \".join(abstracts_list)}\\n')\n",
    "print(\"Saved inverted_file.txt...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dab942",
   "metadata": {},
   "source": [
    "Now we can use the index created by `inverted_file.txt` to create a query search system. First we define the `search_keywords()` method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5164bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keywords(keywords):\n",
    "    results = set()\n",
    "    for keyword in keywords:\n",
    "        keyword = keyword.lower()\n",
    "        if keyword in inverted_file:\n",
    "            results.update(inverted_file[keyword])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca2414",
   "metadata": {},
   "source": [
    "Now we search for `dark, matter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "388626b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Abstract: A5.txt\n",
      "Matching Abstract: A2.txt\n",
      "Matching Abstract: A1.txt\n",
      "Matching Abstract: A24.txt\n",
      "Matching Abstract: A18.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_keywords_list = [\"dark\", \"matter\"]\n",
    "matching_abstracts = search_keywords(search_keywords_list)\n",
    "\n",
    "# Print matching abstracts\n",
    "for abstract in matching_abstracts:\n",
    "    print(f'Matching Abstract: {abstract}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b927d4a",
   "metadata": {},
   "source": [
    "Another query on `black, hole`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7b71c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Abstract: A17.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_keywords_list = [\"black\", \"hole\"]\n",
    "matching_abstracts = search_keywords(search_keywords_list)\n",
    "\n",
    "# Print matching abstracts\n",
    "for abstract in matching_abstracts:\n",
    "    print(f'Matching Abstract: {abstract}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f98ac",
   "metadata": {},
   "source": [
    "## Solution B4:\n",
    "We want to relax the assumption of exact matching between keywords and words of the abstract and allow the matching to be considered correct if 90% of the characters of the keywords are found in one word of the abstract. Write a script that implements this reasoning and display the result of your search operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ff11b",
   "metadata": {},
   "source": [
    "We can implement a fuzzy / partial matching approach without using external libraries by calculating the Levenshtein (edit) distance between keywords and words in the abstract. Here is how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "17e9b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Levenshtein distance between two strings\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "# Function to check if a keyword is a fuzzy match in an abstract word\n",
    "def is_fuzzy_match(keyword, word):\n",
    "    return levenshtein_distance(keyword.lower(), word.lower()) <= max(len(keyword) * 0.1, 1)  # Adjust the threshold as needed\n",
    "\n",
    "# Function to search for keywords with fuzzy matching\n",
    "def search_keywords_fuzzy(keywords):\n",
    "    results = set()\n",
    "    for keyword in keywords:\n",
    "        keyword = keyword.lower()\n",
    "        for i in range(len(abstracts)):\n",
    "            abstract_filename = f'abstracts/A{i}.txt'\n",
    "            with open(abstract_filename, 'r') as abstract_file:\n",
    "                abstract_text = abstract_file.read()\n",
    "                # Tokenize and process words in the abstract\n",
    "                words = abstract_text.split()  # Split by space as a basic example\n",
    "                for word in words:\n",
    "                    if is_fuzzy_match(keyword, word):\n",
    "                        results.add(f'A{i}')\n",
    "                        break  # Break if one match is found in the abstract\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f2755053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Abstract: A5\n",
      "Matching Abstract: A16\n",
      "Matching Abstract: A17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 1: Search for keywords with fuzzy matching\n",
    "search_keywords_list_fuzzy = [\"blak\", \"hol\"]\n",
    "matching_abstracts_fuzzy = search_keywords_fuzzy(search_keywords_list_fuzzy)\n",
    "\n",
    "# Print matching abstracts\n",
    "for abstract in matching_abstracts_fuzzy:\n",
    "    print(f'Matching Abstract: {abstract}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "abb8da0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Abstract: A24\n",
      "Matching Abstract: A2\n",
      "Matching Abstract: A18\n",
      "Matching Abstract: A5\n",
      "Matching Abstract: A1\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Search for keywords with fuzzy matching\n",
    "search_keywords_list_fuzzy = [\"dak\", \"matar\"]\n",
    "matching_abstracts_fuzzy = search_keywords_fuzzy(search_keywords_list_fuzzy)\n",
    "\n",
    "# Print matching abstracts\n",
    "for abstract in matching_abstracts_fuzzy:\n",
    "    print(f'Matching Abstract: {abstract}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba2d119",
   "metadata": {},
   "source": [
    "### End of Task B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
